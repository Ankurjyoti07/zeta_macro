{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from astropy.io import fits\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as skl\n",
    "import pandas as pd\n",
    "import glob\n",
    "import gzip\n",
    "import os\n",
    "from scipy.interpolate import interp1d\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.signal import fftconvolve\n",
    "from PyAstronomy.pyasl import rotBroad\n",
    "import numpy as np\n",
    "from math import sin, pi\n",
    "from scipy.special import erf                               # Error function\n",
    "from scipy.signal import fftconvolve \n",
    "from lmfit import Parameters, minimize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_HERMES(infile):\n",
    "    #print(\"%s: Input file is a HERMES file.\" % infile)\n",
    "    header = fits.getheader(infile)\n",
    "\n",
    "    #bjd = header['MJD-OBS']\n",
    "    # for files with standard wavelegth array\n",
    "    if ((header['CTYPE1'] == 'WAVELENGTH') or (header['CTYPE1'] == 'AWAV')):\n",
    "        flux = fits.getdata(infile, byteorder='little')\n",
    "        crval = header['CRVAL1']\n",
    "        cdelt = header['CDELT1']\n",
    "        naxis1 = header['NAXIS1']\n",
    "        wave = crval + np.arange(0, naxis1) * cdelt\n",
    "\n",
    "    # for files that are given in logarithmic wl array\n",
    "    if (header['CTYPE1'] == 'log(wavelength)'):\n",
    "        flux = fits.getdata(infile, byteorder='little')\n",
    "        crval = header['CRVAL1']\n",
    "        cdelt = header['CDELT1']\n",
    "        naxis1 = header['NAXIS1']\n",
    "        wave = np.exp(crval + np.arange(0, naxis1)*cdelt)\n",
    "    else:\n",
    "        print(\"Could not read in HERMES fits file - unknown file type.\")\n",
    "        sys.exit()\n",
    "    return wave, flux\n",
    "\n",
    "#reading from linelist to fit\n",
    "def read_line_list(filename):\n",
    "    line_centers = []\n",
    "    line_widths = []\n",
    "\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()  #removing whitespaces\n",
    "            if not line:\n",
    "                continue  #skipping empty lines\n",
    "            parts = line.split()\n",
    "            center = float(parts[0])\n",
    "            if len(parts) > 1:\n",
    "                width = float(parts[1])\n",
    "            else:\n",
    "                width = 10.0  #default\n",
    "            line_centers.append(center)\n",
    "            line_widths.append(width)\n",
    "\n",
    "    return line_centers, line_widths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss(x,a,center,R, gamma):\n",
    "  sigma = sigma = 4471/ (2.0 * R * np.sqrt(2.0 * np.log(2))) \n",
    "  return a*np.exp(-(x-center)**2/(2*sigma**2)) + gamma\n",
    "\n",
    "def generate_data(wave, flux, line_centers, line_widths, wavelength_slices):\n",
    "    interp_func = interp1d(wave, flux, kind='linear')\n",
    "    wave_slices = []\n",
    "    flux_slices = []\n",
    "    for center, width in zip(line_centers, line_widths):\n",
    "        new_wave = np.linspace(center - width, center + width, wavelength_slices)\n",
    "        new_flux = interp_func(new_wave)\n",
    "        wave_slices.append(new_wave)\n",
    "        flux_slices.append(new_flux)\n",
    "    return np.concatenate(wave_slices), np.concatenate(flux_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "class Model_broad:\n",
    "    def __init__(self, wave, flux):\n",
    "        self.x = wave\n",
    "        self.y = flux\n",
    "\n",
    "\n",
    "def Broaden(model, vsini, epsilon=0.5, linear=False, findcont=False):\n",
    "    # Remove NaN values from the flux array and corresponding wavelength values\n",
    "    non_nan_idx = ~np.isnan(model.y)\n",
    "    wvl = model.x[non_nan_idx]\n",
    "    flx = model.y[non_nan_idx]\n",
    "    \n",
    "    dwl = wvl[1] - wvl[0]\n",
    "    binnu = int(np.floor((((vsini/10)/ 299792.458) * max(wvl)) / dwl)) + 1 #adding extra bins for error handling\n",
    "    #validIndices = np.arange(len(flx)) + binnu => this was used in rotbroad as a user cond ==> this is always on here\n",
    "    front_fl = np.ones(binnu) * flx[0]\n",
    "    end_fl = np.ones(binnu) * flx[-1]\n",
    "    flux = np.concatenate((front_fl, flx, end_fl))\n",
    "\n",
    "    front_wv = (wvl[0] - (np.arange(binnu) + 1) * dwl)[::-1]\n",
    "    end_wv = wvl[-1] + (np.arange(binnu) + 1) * dwl\n",
    "    wave = np.concatenate((front_wv, wvl, end_wv))\n",
    "\n",
    "    if not linear:\n",
    "        x = np.logspace(np.log10(wave[0]), np.log10(wave[-1]), len(wave))\n",
    "    else:\n",
    "        x = wave\n",
    "        \n",
    "    if findcont:\n",
    "        # Find the continuum\n",
    "        model.cont = np.ones_like(flux)  # Placeholder for continuum finding\n",
    "        \n",
    "    # Make the broadening kernel\n",
    "    dx = np.log(x[1] / x[0])\n",
    "    c = 299792458  # Speed of light in m/s\n",
    "    lim = vsini / c\n",
    "    if lim < dx:\n",
    "        warnings.warn(\"vsini too small ({}). Not broadening!\".format(vsini))\n",
    "        return Model_broad(wave.copy(), flux.copy())  # Create a copy of the Model object\n",
    "    \n",
    "    d_logx = np.arange(0.0, lim, dx)\n",
    "    d_logx = np.concatenate((-d_logx[::-1][:-1], d_logx))\n",
    "    alpha = 1.0 - (d_logx / lim) ** 2\n",
    "    B = (1.0 - epsilon) * np.sqrt(alpha) + epsilon * np.pi * alpha / 4.0  # Broadening kernel\n",
    "    B /= np.sum(B)  # Normalize\n",
    "\n",
    "    # Do the convolution\n",
    "    broadened = Model_broad(wave.copy(), flux.copy())  # Create a copy of the Model object\n",
    "    broadened.y = fftconvolve(flux, B, mode='same')\n",
    "    \n",
    "    return broadened\n",
    "\n",
    "def macro_broaden(xdata, ydata, vmacro):\n",
    "    c = 299792458 #~constants.c.cgs.value * units.cm.to(units.km)\n",
    "    sq_pi = np.sqrt(np.pi)\n",
    "    lambda0 = np.median(xdata)\n",
    "    xspacing = xdata[1] - xdata[0]\n",
    "    mr = vmacro * lambda0 / c\n",
    "    ccr = 2 / (sq_pi * mr)\n",
    "\n",
    "    px = np.arange(-len(xdata) / 2, len(xdata) / 2 + 1) * xspacing\n",
    "    pxmr = abs(px) / mr\n",
    "    profile = ccr * (np.exp(-pxmr ** 2) + sq_pi * pxmr * (erf(pxmr) - 1.0))\n",
    "\n",
    "    before = ydata[int(-profile.size / 2 + 1):]\n",
    "    after = ydata[:int(profile.size / 2 +1)] #add one to fix size mismatch\n",
    "    extended = np.r_[before, ydata, after]\n",
    "\n",
    "    first = xdata[0] - float(int(profile.size / 2.0 + 0.5)) * xspacing\n",
    "    last = xdata[-1] + float(int(profile.size / 2.0 + 0.5)) * xspacing\n",
    "    \n",
    "    x2 = np.linspace(first, last, extended.size)  #newdata x array ==> handles edge effects\n",
    "\n",
    "    conv_mode = \"valid\"\n",
    "\n",
    "    newydata = fftconvolve(extended, profile / profile.sum(), mode=conv_mode)\n",
    "\n",
    "    return newydata\n",
    "\n",
    "\n",
    "def generate_broaden(params, line_centers, line_widths, wavelength_slices):\n",
    "    model_slices = []\n",
    "    for i, (center, width) in enumerate(zip(line_centers, line_widths)):\n",
    "        wave = np.linspace(center - width, center + width, wavelength_slices)\n",
    "        \n",
    "        instrum = gauss(wave, params[f'a{i}'], params[f'center{i}'], 20000, params[f'gamma{i}']) #resolution is still hardcoded R=20000 change accordingly\n",
    "        broad_rot = Broaden(Model_broad(wave, instrum), params['vsini'])\n",
    "        \n",
    "        broad_macro = macro_broaden(broad_rot.x, broad_rot.y, params[f'vmacro{i}']) #macro broad restores the same wave array as input  \n",
    "        \n",
    "        interp = interp1d(broad_rot.x, broad_macro, kind= 'linear')\n",
    "        broad_flux = interp(wave)\n",
    "        model_slices.append(broad_flux)\n",
    "        \n",
    "    return  np.concatenate(model_slices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params, wave, flux, line_centers, line_widths, wavelength_slices):\n",
    "    wave_data, flux_data = generate_data(wave, flux, line_centers, line_widths, wavelength_slices)\n",
    "    model = generate_broaden(params, line_centers, line_widths, wavelength_slices)\n",
    "    return flux_data - model\n",
    "\n",
    "def fit_lines(wave, flux, line_centers, line_widths, wavelength_slices):\n",
    "    params = Parameters()\n",
    "    wave_data, flux_data = generate_data(wave, flux, line_centers, line_widths, wavelength_slices)\n",
    "    for i, (center, width) in enumerate(zip(line_centers, line_widths)):\n",
    "        params.add(f'a{i}', value=-1)   # Initial guess for amplitude\n",
    "        params.add(f'center{i}', value=center)  # Initial guess for center\n",
    "        params.add(f'gamma{i}', value=1)\n",
    "        params.add(f'vmacro{i}', value=150000, min = 0, max = 500000)\n",
    "    params.add('vsini', value=150000, min = 0, max = 500000)\n",
    "    \n",
    "\n",
    "    result = minimize(objective, params=params, args=(wave_data, flux_data, line_centers, line_widths, wavelength_slices))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>Fit Result</h2> <table class=\"jp-toc-ignore\"><caption class=\"jp-toc-ignore\">Fit Statistics</caption><tr><td style='text-align:left'>fitting method</td><td style='text-align:right'>leastsq</td></tr><tr><td style='text-align:left'># function evals</td><td style='text-align:right'>206</td></tr><tr><td style='text-align:left'># data points</td><td style='text-align:right'>3000</td></tr><tr><td style='text-align:left'># variables</td><td style='text-align:right'>13</td></tr><tr><td style='text-align:left'>chi-square</td><td style='text-align:right'> 0.20778577</td></tr><tr><td style='text-align:left'>reduced chi-square</td><td style='text-align:right'> 6.9563e-05</td></tr><tr><td style='text-align:left'>Akaike info crit.</td><td style='text-align:right'>-28706.8458</td></tr><tr><td style='text-align:left'>Bayesian info crit.</td><td style='text-align:right'>-28628.7630</td></tr></table><table class=\"jp-toc-ignore\"><caption>Parameters</caption><tr><th style='text-align:left'>name</th><th style='text-align:left'>value</th><th style='text-align:left'>standard error</th><th style='text-align:left'>relative error</th><th style='text-align:left'>initial value</th><th style='text-align:left'>min</th><th style='text-align:left'>max</th><th style='text-align:right'>vary</th></tr><tr><td style='text-align:left'>a0</td><td style='text-align:left'>-3.33132928</td><td style='text-align:left'> 0.02792668</td><td style='text-align:left'>(0.84%)</td><td style='text-align:left'>-1</td><td style='text-align:left'>       -inf</td><td style='text-align:left'>        inf</td><td style='text-align:right'>True</td></tr><tr><td style='text-align:left'>center0</td><td style='text-align:left'> 4471.19029</td><td style='text-align:left'> 0.01692185</td><td style='text-align:left'>(0.00%)</td><td style='text-align:left'>4471.0</td><td style='text-align:left'>       -inf</td><td style='text-align:left'>        inf</td><td style='text-align:right'>True</td></tr><tr><td style='text-align:left'>gamma0</td><td style='text-align:left'> 0.99108403</td><td style='text-align:left'> 3.6710e-04</td><td style='text-align:left'>(0.04%)</td><td style='text-align:left'>1</td><td style='text-align:left'>       -inf</td><td style='text-align:left'>        inf</td><td style='text-align:right'>True</td></tr><tr><td style='text-align:left'>vmacro0</td><td style='text-align:left'> 133416.634</td><td style='text-align:left'> 7390.01604</td><td style='text-align:left'>(5.54%)</td><td style='text-align:left'>150000</td><td style='text-align:left'> 0.00000000</td><td style='text-align:left'> 500000.000</td><td style='text-align:right'>True</td></tr><tr><td style='text-align:left'>a1</td><td style='text-align:left'>-1.56311069</td><td style='text-align:left'> 0.02510092</td><td style='text-align:left'>(1.61%)</td><td style='text-align:left'>-1</td><td style='text-align:left'>       -inf</td><td style='text-align:left'>        inf</td><td style='text-align:right'>True</td></tr><tr><td style='text-align:left'>center1</td><td style='text-align:left'> 5875.67269</td><td style='text-align:left'> 0.03913025</td><td style='text-align:left'>(0.00%)</td><td style='text-align:left'>5875.0</td><td style='text-align:left'>       -inf</td><td style='text-align:left'>        inf</td><td style='text-align:right'>True</td></tr><tr><td style='text-align:left'>gamma1</td><td style='text-align:left'> 1.01670977</td><td style='text-align:left'> 3.7832e-04</td><td style='text-align:left'>(0.04%)</td><td style='text-align:left'>1</td><td style='text-align:left'>       -inf</td><td style='text-align:left'>        inf</td><td style='text-align:right'>True</td></tr><tr><td style='text-align:left'>vmacro1</td><td style='text-align:left'> 24633.0029</td><td style='text-align:left'> 16818.9449</td><td style='text-align:left'>(68.28%)</td><td style='text-align:left'>150000</td><td style='text-align:left'> 0.00000000</td><td style='text-align:left'> 500000.000</td><td style='text-align:right'>True</td></tr><tr><td style='text-align:left'>a2</td><td style='text-align:left'>-1.40256514</td><td style='text-align:left'> 0.03170308</td><td style='text-align:left'>(2.26%)</td><td style='text-align:left'>-1</td><td style='text-align:left'>       -inf</td><td style='text-align:left'>        inf</td><td style='text-align:right'>True</td></tr><tr><td style='text-align:left'>center2</td><td style='text-align:left'> 6678.13428</td><td style='text-align:left'> 0.05266768</td><td style='text-align:left'>(0.00%)</td><td style='text-align:left'>6678.0</td><td style='text-align:left'>       -inf</td><td style='text-align:left'>        inf</td><td style='text-align:right'>True</td></tr><tr><td style='text-align:left'>gamma2</td><td style='text-align:left'> 1.01255419</td><td style='text-align:left'> 4.5996e-04</td><td style='text-align:left'>(0.05%)</td><td style='text-align:left'>1</td><td style='text-align:left'>       -inf</td><td style='text-align:left'>        inf</td><td style='text-align:right'>True</td></tr><tr><td style='text-align:left'>vmacro2</td><td style='text-align:left'> 40635.2195</td><td style='text-align:left'> 18291.2392</td><td style='text-align:left'>(45.01%)</td><td style='text-align:left'>150000</td><td style='text-align:left'> 0.00000000</td><td style='text-align:left'> 500000.000</td><td style='text-align:right'>True</td></tr><tr><td style='text-align:left'>vsini</td><td style='text-align:left'> 289767.179</td><td style='text-align:left'> 1652.96858</td><td style='text-align:left'>(0.57%)</td><td style='text-align:left'>150000</td><td style='text-align:left'> 0.00000000</td><td style='text-align:left'> 500000.000</td><td style='text-align:right'>True</td></tr></table><table class=\"jp-toc-ignore\"><caption>Correlations (unreported values are < 0.100)</caption><tr><th style='text-align:left'>Parameter1</th><th style='text-align:left'>Parameter 2</th><th style='text-align:right'>Correlation</th></tr><tr><td style='text-align:left'>a2</td><td style='text-align:left'>gamma2</td><td style='text-align:right'>-0.8193</td></tr><tr><td style='text-align:left'>a1</td><td style='text-align:left'>gamma1</td><td style='text-align:right'>-0.7169</td></tr><tr><td style='text-align:left'>a0</td><td style='text-align:left'>gamma0</td><td style='text-align:right'>-0.6956</td></tr><tr><td style='text-align:left'>a0</td><td style='text-align:left'>vmacro0</td><td style='text-align:right'>-0.5965</td></tr><tr><td style='text-align:left'>a2</td><td style='text-align:left'>vmacro2</td><td style='text-align:right'>-0.4882</td></tr><tr><td style='text-align:left'>vmacro0</td><td style='text-align:left'>vsini</td><td style='text-align:right'>-0.4555</td></tr><tr><td style='text-align:left'>gamma0</td><td style='text-align:left'>vmacro0</td><td style='text-align:right'>+0.4149</td></tr><tr><td style='text-align:left'>gamma2</td><td style='text-align:left'>vmacro2</td><td style='text-align:right'>+0.3999</td></tr><tr><td style='text-align:left'>a1</td><td style='text-align:left'>vsini</td><td style='text-align:right'>-0.2929</td></tr><tr><td style='text-align:left'>a2</td><td style='text-align:left'>vsini</td><td style='text-align:right'>-0.2726</td></tr><tr><td style='text-align:left'>a1</td><td style='text-align:left'>vmacro1</td><td style='text-align:right'>-0.2550</td></tr><tr><td style='text-align:left'>gamma2</td><td style='text-align:left'>vsini</td><td style='text-align:right'>+0.2233</td></tr><tr><td style='text-align:left'>vmacro1</td><td style='text-align:left'>vsini</td><td style='text-align:right'>-0.2131</td></tr><tr><td style='text-align:left'>gamma1</td><td style='text-align:left'>vsini</td><td style='text-align:right'>+0.2100</td></tr><tr><td style='text-align:left'>gamma1</td><td style='text-align:left'>vmacro1</td><td style='text-align:right'>+0.1828</td></tr><tr><td style='text-align:left'>vmacro0</td><td style='text-align:left'>a1</td><td style='text-align:right'>+0.1334</td></tr><tr><td style='text-align:left'>vmacro0</td><td style='text-align:left'>a2</td><td style='text-align:right'>+0.1242</td></tr><tr><td style='text-align:left'>vmacro0</td><td style='text-align:left'>gamma2</td><td style='text-align:right'>-0.1017</td></tr></table>"
      ],
      "text/plain": [
       "<lmfit.minimizer.MinimizerResult at 0x7821dfeed850>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wave, flux = read_HERMES('00943975_HRF_OBJ_ext_CosmicsRemoved_log_merged_cf_norm.fits')\n",
    "line_centers, line_widths = read_line_list('line_list.txt')\n",
    "result = fit_lines(wave, flux, line_centers, line_widths, wavelength_slices=1000)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### errors with emcee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "import corner\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the log likelihood function\n",
    "def log_likelihood(params, wave, flux, line_centers, line_widths, wavelength_slices):\n",
    "    wave_data, flux_data = generate_data(wave, flux, line_centers, line_widths, wavelength_slices)\n",
    "    model = generate_broaden(params, line_centers, line_widths, wavelength_slices)\n",
    "    residuals = flux_data - model\n",
    "    # Assuming Gaussian uncertainties, calculate the log likelihood\n",
    "    sigma = np.sqrt(np.mean(residuals**2))  # Assuming constant uncertainty for simplicity\n",
    "    log_likelihood = -0.5 * np.sum((residuals / sigma)**2)\n",
    "    return log_likelihood\n",
    "\n",
    "# Define the log prior function\n",
    "def log_prior(params):\n",
    "    # Assuming flat priors for simplicity\n",
    "    # Check if parameters are within allowed ranges\n",
    "    for i, value in enumerate(params):\n",
    "        key = list(result.params.keys())[i]\n",
    "        if key.startswith('vmacro'):\n",
    "            if value < 0 or value > 500000:\n",
    "                return -np.inf\n",
    "        elif key == 'vsini':\n",
    "            if value < 0 or value > 500000:\n",
    "                return -np.inf\n",
    "    return 0.0\n",
    "\n",
    "# Define the log probability function\n",
    "def log_probability(params, wave, flux, line_centers, line_widths, wavelength_slices):\n",
    "    lp = log_prior(params)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + log_likelihood(params, wave, flux, line_centers, line_widths, wavelength_slices)\n",
    "\n",
    "# Set up the initial positions of the walkers\n",
    "nwalkers = 32  # Number of walkers\n",
    "ndim = len(result.params)  # Dimensionality of the parameter space\n",
    "initial_params = [np.array(list(result.params.valuesdict().values())) + 1e-4 * np.random.randn(ndim) for _ in range(nwalkers)]\n",
    "\n",
    "# Set up the emcee sampler\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_probability, args=(wave, flux, line_centers, line_widths, 1000))\n",
    "\n",
    "# Run the MCMC sampler with progress indicators\n",
    "nsteps = 500  # Number of steps\n",
    "with tqdm(total=nsteps) as pbar:\n",
    "    for _ in sampler.sample(initial_params, iterations=nsteps):\n",
    "        pbar.update(1)\n",
    "\n",
    "# Extract the chain and discard burn-in\n",
    "chain = sampler.get_chain()\n",
    "burnin = 100  # Number of burn-in steps\n",
    "samples = sampler.get_chain(discard=burnin, flat=True)\n",
    "\n",
    "# Plot the results using the corner plot\n",
    "labels = [f\"{param}\" for param in result.params.keys()]\n",
    "corner.corner(samples, labels=labels, truths=list(result.params.valuesdict().values()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
